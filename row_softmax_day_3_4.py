# -*- coding: utf-8 -*-
"""row-softmax-day-3-4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1txduCW3wN8moSdMsWc-w5buZDU-RacyP
"""

# !pip install triton==3.0.0
# !nvidia-smi

import torch
import triton
import triton.language as tl

@triton.jit
def softmax_row_no_mask(X_ptr, Y_ptr, BLOCK: tl.constexpr):
    # offsets: positions 0..BLOCK-1 in the row
    offs = tl.arange(0, BLOCK)

    # load the whole row as a vector
    x = tl.load(X_ptr + offs).to(tl.float32)

    # numerically stable softmax
    m = tl.max(x, axis=0)
    ex = tl.exp(x - m)
    denom = tl.sum(ex, axis=0)
    y = ex / denom

    # write back
    tl.store(Y_ptr + offs, y)

def softmax_one_row_simple(x: torch.Tensor) -> torch.Tensor:
    assert x.is_cuda and x.dim() == 1, "pass a 1D CUDA tensor"
    N = x.numel()
    y = torch.empty_like(x, dtype=torch.float32)

    # BLOCK == N ‚Üí no masks, no loops
    softmax_row_no_mask[(1,)](x, y, BLOCK=N, num_warps=1)
    return y

# ---- Quick tests ----
device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)

# Example 1: tiny row
x = torch.tensor([2., 1., 0., 8.], device=device, dtype=torch.float32)
y_triton = softmax_one_row_simple(x)
y_torch  = torch.softmax(x, dim=0)
print("x:", x.tolist())
print("triton:", [float(v) for v in y_triton])
print("torch :", [float(v) for v in y_torch])
print("max abs diff:", float((y_triton - y_torch).abs().max()))









"""
---

## üìå Code Comment / Docstring for `row_start` and `col_offsets`

**Example:**
Matrix shape = **M=3 rows √ó N=4 columns**
Stored in **row-major** (PyTorch default, contiguous)
Memory layout (flattened):

| Flat index | Element |
| ---------- | ------- |
| 0          | a00     |
| 1          | a01     |
| 2          | a02     |
| 3          | a03     |
| 4          | a10     |
| 5          | a11     |
| 6          | a12     |
| 7          | a13     |
| 8          | a20     |
| 9          | a21     |
| 10         | a22     |
| 11         | a23     |

---

### **1Ô∏è‚É£ Why `row_start = row_id * N`**

* **N** = number of columns = row stride in contiguous row-major layout.
* In flat memory, each new row starts `N` elements after the previous one.

  * Row 0 start = `0 * 4 = 0`
  * Row 1 start = `1 * 4 = 4`
  * Row 2 start = `2 * 4 = 8`
* So for `row_id=1`, `row_start = 4` ‚Üí points to `a10` in memory.

---

### **2Ô∏è‚É£ Why `col_offsets = tl.arange(0, BLOCK_SIZE)`**

* Generates a **vector** of column indices inside the tile.
  Example: `BLOCK_SIZE = 4` ‚Üí `col_offsets = [0, 1, 2, 3]`
* Allows Triton to load/store multiple elements **in parallel** (one per lane).

---

### **3Ô∏è‚É£ Why `X_ptr + row_start + col_offsets`**

* `X_ptr` ‚Üí pointer to the start of the entire matrix.
* `row_start` ‚Üí jump to the start of the current row.
* `col_offsets` ‚Üí move to the correct columns within this row‚Äôs tile.
* Adding them gives the **vector of addresses** for the tile:

  * For `row_id=1`, `row_start=4`, `col_offsets=[0,1,2,3]` ‚Üí `[4,5,6,7]`
    ‚Üí corresponds to `[a10, a11, a12, a13]` in flat memory.

---

**Summary formula for flat memory addressing in Triton:**

```
addresses_for_tile = base_ptr + row_id * N + col_offsets
```

* `row_id * N` ‚Üí stride to the correct row start.
* `col_offsets` ‚Üí stride within the row to pick the tile‚Äôs columns.
* Works because **PyTorch tensors are row-major by default**.

---"""

import torch
import triton
import triton.language as tl

@triton.jit
def softmax_kernel(X_ptr, Y_ptr, M, N, BLOCK_SIZE:  tl.constexpr):

    # Row index for this program
    row_id = tl.program_id(0)

    # since pytorch stores address in Row Major Format
    row_start = row_id * BLOCK_SIZE

    # Column offsets for the BLOCK
    col_offsets = tl.arange(0, BLOCK_SIZE)

    # Pointers to this row's data
    X_row_ptr = X_ptr + row_start + col_offsets
    Y_row_ptr = Y_ptr + row_start + col_offsets

    # Load row into registers (no masking here)
    x = tl.load(X_row_ptr)

    # Step 1: Find max for numerical stability
    row_max = tl.max(x, axis=0)
    x = x - row_max

    # Step 2: Exponentiate
    exp_x = tl.exp(x)

    # Step 3: Sum all exponentials
    row_sum = tl.sum(exp_x, axis=0)

    # Step 4: Divide to get softmax
    y = exp_x / row_sum

    # Store the results
    tl.store(Y_row_ptr, y)


def softmax_triton(X):
    M, N = X.shape
    Y = torch.empty_like(X)
    # Launch M programs, one per row
    softmax_kernel[(M,)](X, Y, M, N, BLOCK_SIZE=N)
    return Y


# Example
M, N = 4, 8
X = torch.randn((M, N), device='cuda', dtype=torch.float32)
Y_triton = softmax_triton(X)

# PyTorch softmax for comparison
Y_torch = torch.softmax(X, dim=1)


# torch.allclose is a PyTorch function used to determine if two tensors are element-wise equal within a specified
# tolerance.
print(torch.allclose(Y_triton, Y_torch, atol=1e-6))

