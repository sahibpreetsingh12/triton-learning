# -*- coding: utf-8 -*-
"""practice_day1-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ft9_CPQeIgs76opymduGZ5dn_iq-N6uo
"""

!pip install triton
!nvidia-smi

import triton
import triton.language as tl
import torch

"""# Scale each element by a constant
# Input: X = [1, 2, 3, 4], scale = 2.5
# Output: [2.5, 5.0, 7.5, 10.0]

## For scaling a vector
"""

@triton.jit
def scale_vector(input_ptr, scale_factor, output_ptr, N, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N

    # Load the input block
    input_block = tl.load(input_ptr + offsets, mask=mask, other=0.0)

    # Scale it (no need for intermediate variable)
    scaled_block = input_block * scale_factor

    # Store the result
    tl.store(output_ptr + offsets, scaled_block, mask=mask)

"""# Sum each row of a 2D matrix
# Input: X =
[[1, 2, 3],

[4, 5, 6]]
# Output: [6, 15]
"""

@triton.jit
def sum_rows(input_ptr, output_ptr, M, N, stride_m, stride_n, BLOCK_SIZE: tl.constexpr):
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)

    offsets_m = pid_m * BLOCK_SIZE +tl.arange(0, BLOCK_SIZE)
    mask_m = offsets_m < N


    offsets_n = pid_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask_n = offsets_n < M

    input_row = input_ptr + (offsets_m[:, None] * stride_m + offsets_n[None, :]*stride_n)
    input_block = tl.load(input_row, mask=mask_m[:, None], other=0.0)

    output_vector = sum(input_block, axis=0)
    output_row = output_ptr + pid_m
    tl.store(output_row, output_vector, mask=mask_n)

"""# Sum each row of a 2D matrix
# Input: X =

[[1, 2, 3],

[4, 5, 6]]     


Output: [6, 15]
"""

@triton.jit
def sum_rows(input_ptr, output_ptr, M, N, stride_m, stride_n, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)

    # Which rows does this program handle?
    row_offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    row_mask = row_offsets < M  # ← This should be M, not N!

    # For each of my rows, I need ALL columns [0, 1, 2, ..., N-1]
    col_offsets = tl.arange(0, N)

    # Create 2D pointer grid: [BLOCK_SIZE, N]
    ptrs = input_ptr + row_offsets[:, None] * stride_m + col_offsets[None, :] * stride_n

    # Load my block of rows (complete rows)
    values = tl.load(ptrs, mask=row_mask[:, None], other=0.0)

    # Sum each row separately (sum along axis 1 = columns)
    row_sums = tl.sum(values, axis=1)

    # Store results for my rows
    tl.store(output_ptr + row_offsets, row_sums, mask=row_mask)

"""## Transpose"""

import torch
import triton
import triton.language as tl

@triton.jit
def transpose_kernel(input_ptr, output_ptr, M, input_stride_m, input_stride_n, output_stride_m, output_stride_n, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)

    # Which input rows does this program handle?
    input_row_offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    input_row_mask = input_row_offsets < M

    # For N=3, use next power of 2 which is 4
    input_col_offsets = tl.arange(0, 4)  # Power of 2
    input_col_mask = input_col_offsets < 3  # Mask out column 3

    # Create 2D pointer grid for INPUT: [BLOCK_SIZE, 4]
    input_ptrs = input_ptr + input_row_offsets[:, None] * input_stride_m + input_col_offsets[None, :] * input_stride_n

    # Load with combined mask
    combined_mask = input_row_mask[:, None] & input_col_mask[None, :]
    input_values = tl.load(input_ptrs, mask=combined_mask, other=0.0)

    # Transpose the values: [BLOCK_SIZE, 4] → [4, BLOCK_SIZE]
    transposed_values = tl.trans(input_values)

    # Now store to output: my input rows become output columns
    output_col_offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    output_col_mask = output_col_offsets < M

    # For output, use power of 2 for N=3 → use 4
    output_row_offsets = tl.arange(0, 4)  # Power of 2
    output_row_mask = output_row_offsets < 3  # Mask out row 3

    # Create output pointers: [4, BLOCK_SIZE]
    output_ptrs = output_ptr + output_row_offsets[:, None] * output_stride_m + output_col_offsets[None, :] * output_stride_n

    # Store with combined mask
    combined_output_mask = output_row_mask[:, None] & output_col_mask[None, :]
    tl.store(output_ptrs, transposed_values, mask=combined_output_mask)

# Test it
X = torch.tensor([[1.0, 2.0, 3.0],
                  [4.0, 5.0, 6.0]], device='cuda')

M, N = X.shape
Y = torch.empty((N, M), device='cuda', dtype=X.dtype)

BLOCK_SIZE = 2  # Power of 2
grid = (1,)     # 1 program handles both rows

transpose_kernel[grid](
    X, Y, M,
    X.stride(0), X.stride(1),
    Y.stride(0), Y.stride(1),
    BLOCK_SIZE
)

print("Input:")
print(X)
print("Output:")
print(Y)



