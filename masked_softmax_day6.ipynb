{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "here’s a clean **blog-ready note** you can drop in as a section 👇\n",
        "\n",
        "---\n",
        "\n",
        "# Masked Softmax: What It Is and Why We Need It\n",
        "\n",
        "## 🔹 What is Softmax?\n",
        "\n",
        "Softmax takes a row of numbers (logits) and turns them into probabilities:\n",
        "\n",
        "* Exponentiate each number.\n",
        "* Divide by the sum.\n",
        "* End result: all values are between 0 and 1, and they sum to 1.\n",
        "\n",
        "This is how transformers turn attention scores into a probability distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 What is Masked Softmax?\n",
        "\n",
        "In real models, not every position should be included in that distribution.\n",
        "**Masked softmax** = normal softmax, but with some positions **hidden** (set to `-∞` before exponentiation).\n",
        "That makes their probability exactly **0**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 Why Padding Mask?\n",
        "\n",
        "* In a batch, sequences have different lengths.\n",
        "* We pad shorter sequences with `<pad>` tokens so everything fits into the same rectangular tensor.\n",
        "* But the model must not “pay attention” to padding — those are fake tokens.\n",
        "* Padding mask ensures padded positions are excluded (prob = 0).\n",
        "\n",
        "👉 Analogy: Ignore the empty chairs in a classroom when dividing chocolates.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 Why Causal Mask?\n",
        "\n",
        "* In language generation, token at position *i* should only attend to tokens ≤ *i*.\n",
        "* It must not “see the future” (tokens at positions *j > i*).\n",
        "* Causal mask zeros out all attention weights to the right.\n",
        "\n",
        "👉 Analogy: In an exam, you can only look at your notes or what earlier classmates wrote, not peek at future answers.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 Summary\n",
        "\n",
        "* **Softmax** = distribution over all elements.\n",
        "* **Masked softmax** = same, but some entries are forced to zero.\n",
        "* **Padding mask** → ignores fake tokens (padding).\n",
        "* **Causal mask** → enforces left-to-right generation.\n",
        "\n",
        "Masked softmax is essential: without it, transformers would either waste attention on meaningless padding or cheat by looking ahead.\n",
        "\n",
        "---\n",
        "\n",
        "would you like me to also add a **mini numeric example (with logits → probs)** in this note, so readers *see* how masking changes the distribution?\n"
      ],
      "metadata": {
        "id": "pgomVbrpxV5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(\"assets/Day-6-softmax-trick.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH_eedUn4vbY",
        "outputId": "e724960d-f6e5-47e5-8957-6c59396a0c47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9ihYpghxbRw",
        "outputId": "3a2073b6-8079-406d-8c5a-58daaf12dfc8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Sat Aug 16 23:02:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ],
      "metadata": {
        "id": "Vm0GFgQCxiFb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0MGB0NxdxRQN"
      },
      "outputs": [],
      "source": [
        "# Masked Softmax in Triton (padding + causal), numerically stable\n",
        "# - pad_mask: uint8/bool with 1=True(keep), 0=False(mask out)\n",
        "# - causal:   bool; if True, forbid attending to future positions\n",
        "# Works with contiguous or strided tensors via strides.\n",
        "\n",
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def softmax_rows_masked_kernel(\n",
        "    X_ptr, Y_ptr,\n",
        "    M, N,\n",
        "    stride_xm, stride_xn,\n",
        "    stride_ym, stride_yn,\n",
        "    PAD_ptr, stride_pm, stride_pn,\n",
        "    has_pad: tl.constexpr,\n",
        "    causal: tl.constexpr,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid_m = tl.program_id(0)\n",
        "    if pid_m >= M:\n",
        "        return\n",
        "\n",
        "    offs_n = tl.arange(0, BLOCK_SIZE)\n",
        "    valid = offs_n < N\n",
        "\n",
        "    if has_pad:\n",
        "        pad_row = PAD_ptr + pid_m * stride_pm + offs_n * stride_pn\n",
        "        pad_load = tl.load(pad_row, mask=valid, other=0)       # uint8 0/1\n",
        "        pad_ok = pad_load.to(tl.int1)\n",
        "        valid = valid & pad_ok\n",
        "\n",
        "    if causal:\n",
        "        causal_ok = offs_n <= pid_m\n",
        "        valid = valid & causal_ok\n",
        "\n",
        "    row_x = X_ptr + pid_m * stride_xm + offs_n * stride_xn\n",
        "    row_y = Y_ptr + pid_m * stride_ym + offs_n * stride_yn\n",
        "\n",
        "    x = tl.load(row_x, mask=valid, other=-float(\"inf\")).to(tl.float32)\n",
        "    x_max = tl.max(x, axis=0)\n",
        "\n",
        "    # just a mathematical trick to stable\n",
        "    x = x - x_max\n",
        "    num = tl.exp(x)\n",
        "    den = tl.sum(num, axis=0)\n",
        "\n",
        "    # avoid div-by-zero if an entire row is masked (rare, but safe)\n",
        "    den = tl.where(den == 0, 1.0, den)\n",
        "\n",
        "    y = num / den\n",
        "    # write zeros to masked lanes too\n",
        "    y = tl.where(valid, y, 0.0)\n",
        "    tl.store(row_y, y)\n",
        "\n",
        "\n",
        "\n",
        "def softmax_rows_masked(\n",
        "    x: torch.Tensor,\n",
        "    pad_mask: torch.Tensor | None = None,  # [M,N], uint8/bool; 1=True(keep), 0=False(mask)\n",
        "    causal: bool = False,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute masked softmax over last dim for 2D tensor x [M,N].\n",
        "    - pad_mask: positions with 0/False are excluded (prob=0).\n",
        "    - causal=True: for row i, only columns <= i are included.\n",
        "    Returns tensor with same dtype/device as x.\n",
        "    \"\"\"\n",
        "    assert x.dim() == 2, \"expected 2D tensor [M,N]\"\n",
        "    M, N = x.shape\n",
        "    out = torch.empty_like(x)\n",
        "\n",
        "    # choose BLOCK_SIZE as next power-of-two up to 1024\n",
        "    def next_pow2(v: int) -> int:\n",
        "        return 1 << (v - 1).bit_length()\n",
        "    BLOCK_SIZE = min(1024, next_pow2(N))\n",
        "\n",
        "    # strides for layout-agnostic indexing\n",
        "    sxm, sxn = x.stride(0), x.stride(1)\n",
        "    sym, syn = out.stride(0), out.stride(1)\n",
        "\n",
        "    # pad mask setup\n",
        "    has_pad = pad_mask is not None\n",
        "    if has_pad:\n",
        "        assert pad_mask.shape == (M, N), \"pad_mask must be shape [M,N]\"\n",
        "        if pad_mask.dtype is not torch.uint8:\n",
        "            # allow bool masks, but convert to uint8 (0/1) for clean loads\n",
        "            if pad_mask.dtype is torch.bool:\n",
        "                pad_mask = pad_mask.to(torch.uint8)\n",
        "            else:\n",
        "                pad_mask = pad_mask.to(torch.uint8)\n",
        "        pad_mask = pad_mask.to(device=x.device)\n",
        "        PAD_ptr = pad_mask\n",
        "        spm, spn = pad_mask.stride(0), pad_mask.stride(1)\n",
        "    else:\n",
        "        # Dummy values; not used when has_pad=False (branch is erased at compile-time)\n",
        "        PAD_ptr = x\n",
        "        spm, spn = 0, 0\n",
        "\n",
        "    # launch: one program per row\n",
        "    grid = (triton.cdiv(M, 1),)\n",
        "\n",
        "    softmax_rows_masked_kernel[grid](\n",
        "        x, out,\n",
        "        M, N,\n",
        "        sxm, sxn,\n",
        "        sym, syn,\n",
        "        PAD_ptr, spm, spn,\n",
        "        has_pad=has_pad,\n",
        "        causal=causal,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "        # simple heuristic; you can autotune later\n",
        "        num_warps=1 if BLOCK_SIZE <= 128 else 2 if BLOCK_SIZE <= 256 else 4 if BLOCK_SIZE <= 512 else 8,\n",
        "    )\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "M, N = 8, 8\n",
        "x = torch.randn(M, N, device=device, dtype=torch.float16)\n",
        "\n",
        "# 1) no masks → should match torch\n",
        "y = softmax_rows_masked(x)\n",
        "ref = torch.softmax(x.float(), dim=-1).to(x.dtype)\n",
        "torch.testing.assert_close(y, ref, rtol=1e-2, atol=1e-2)\n",
        "\n",
        "# 2) padding mask: zero out last 3 cols\n",
        "pad = torch.ones(M, N, device=device, dtype=torch.uint8)\n",
        "pad[:, -3:] = 0\n",
        "y2 = softmax_rows_masked(x, pad_mask=pad)\n",
        "assert torch.allclose(y2[:, -3:], torch.zeros_like(y2[:, -3:])), \"masked columns must be 0\"\n",
        "# and remaining probs in each row sum ~1\n",
        "sums2 = y2[:, :-3].float().sum(dim=-1)\n",
        "assert torch.allclose(sums2, torch.ones_like(sums2), rtol=1e-3, atol=1e-3)\n",
        "\n",
        "# 3) causal mask (lower triangular)\n",
        "y3 = softmax_rows_masked(x, causal=True)\n",
        "upper = y3.triu(1)  # strictly upper triangle must be ~0\n",
        "assert torch.allclose(upper, torch.zeros_like(upper), atol=1e-6)\n"
      ],
      "metadata": {
        "id": "rpx3n3ZcFSH9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking against Pytorch"
      ],
      "metadata": {
        "id": "hsgDH0q7LF_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, time\n",
        "import triton, triton.language as tl\n",
        "\n",
        "device = \"cuda\"\n",
        "M, N = 4096, 2048   # big enough to show GPU speedup\n",
        "x = torch.randn(M, N, device=device, dtype=torch.float16)\n",
        "\n",
        "# optional pad mask (like masking last k cols)\n",
        "pad = torch.ones(M, N, device=device, dtype=torch.bool)\n",
        "pad[:, -256:] = False\n"
      ],
      "metadata": {
        "id": "0RsICTKKHvkL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Since both triton and pytorch need warmup for JIT/driver Overhead"
      ],
      "metadata": {
        "id": "an1p35l_LK6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    _ = torch.softmax(x.float(), dim=-1)\n",
        "    _ = softmax_rows_masked(x, pad_mask=pad, causal=False)\n",
        "torch.cuda.synchronize()\n"
      ],
      "metadata": {
        "id": "VXPuvp9_LIuV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark(fn, *args, n_warmup=10, n_iter=100):\n",
        "    # warmup\n",
        "    for _ in range(n_warmup):\n",
        "        fn(*args)\n",
        "    torch.cuda.synchronize()\n",
        "    # timed\n",
        "    start = time.time()\n",
        "    for _ in range(n_iter):\n",
        "        fn(*args)\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    return (end-start) / n_iter\n"
      ],
      "metadata": {
        "id": "PIN8n6-HLSpZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch baseline\n",
        "torch_time = benchmark(lambda x: torch.softmax(x.float(), dim=-1), x)\n",
        "\n",
        "# Triton kernel\n",
        "triton_time = benchmark(lambda x: softmax_rows_masked(x, pad_mask=pad, causal=False), x)\n",
        "\n",
        "print(f\"PyTorch softmax: {torch_time*1e3:.3f} ms\")\n",
        "print(f\"Triton softmax : {triton_time*1e3:.3f} ms\")\n",
        "print(f\"Speedup        : {torch_time/triton_time:.2f}×\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5y3-KNVLUm8",
        "outputId": "b2459db3-c2c5-4843-b3f1-4901c36e1399"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch softmax: 0.989 ms\n",
            "Triton softmax : 0.404 ms\n",
            "Speedup        : 2.45×\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o1QEMLbMLZSA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MfXPEWqKLwLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}