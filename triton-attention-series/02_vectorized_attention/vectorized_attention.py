"""
Blog Post 2: Vectorized Attention in Triton - Performance Through Parallelism
============================================================================

Building on our basic implementation, this post demonstrates the power of vectorization
in GPU kernels. We'll eliminate redundant memory operations and leverage GPU parallelism.

Key Optimizations Covered:
- Vectorized memory access patterns (load multiple elements at once)
- Broadcasting for parallel computation
- Memory coalescing improvements
- Elimination of inefficient loops

Performance Goal: 2-4x speedup over basic version
"""

import torch
import triton
import triton.language as tl
import math
import time
import matplotlib.pyplot as plt
import numpy as np
from dataclasses import dataclass
from typing import Tuple
import sys
import os

# Add the basic attention directory to path so we can import it
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_basic_attention'))
from basic_attention import basic_attention


# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class VectorizedConfig:
    """Configuration for vectorized attention kernel"""
    BLOCK_SIZE_SEQ: int = 128    # How many sequence positions to process together
    BLOCK_SIZE_DIM: int = 128    # How many dimensions to process together

    def adjust_for_problem_size(self, seq_len: int, d_model: int) -> Tuple[int, int]:
        """Dynamically adjust block sizes based on actual problem dimensions"""
        # Ensure block sizes are at least as large as the problem
        block_seq = self.BLOCK_SIZE_SEQ
        while block_seq < seq_len:
            block_seq *= 2

        block_dim = self.BLOCK_SIZE_DIM
        while block_dim < d_model:
            block_dim *= 2

        return block_seq, block_dim


# ============================================================================
# VECTORIZED KERNEL - The Star of This Blog Post
# ============================================================================

@triton.jit
def vectorized_attention_kernel(
    Q_ptr, K_ptr, V_ptr, Output_ptr,
    seq_len, d_model, scale,
    BLOCK_SIZE_SEQ: tl.constexpr,
    BLOCK_SIZE_DIM: tl.constexpr
):
    """
    Vectorized attention kernel - still one thread per query, but MUCH more efficient!

    Key Innovation: Instead of loading keys/values one at a time in loops,
    we load ALL of them at once using vectorized operations.

    The Magic:
    - Uses broadcasting to compute multiple dot products simultaneously
    - Eliminates redundant memory loads through vectorization
    - Better utilizes GPU memory bandwidth

    Memory Access Pattern:
    - OLD: Load K[0], compute, Load K[1], compute, ... (seq_len separate loads)
    - NEW: Load ALL K at once, compute ALL scores at once (1 vectorized load)
    """

    # Step 1: Same thread assignment as basic version
    query_idx = tl.program_id(0)
    if query_idx >= seq_len:
        return

    # Step 2: Set up vectorized indexing
    # These create the "coordinate system" for our vectorized operations
    seq_offsets = tl.arange(0, BLOCK_SIZE_SEQ)  # [0, 1, 2, ..., 127] - sequence positions
    dim_offsets = tl.arange(0, BLOCK_SIZE_DIM)  # [0, 1, 2, ..., 127] - feature dimensions

    # Masks to handle variable sizes
    seq_mask = seq_offsets < seq_len
    dim_mask = dim_offsets < d_model

    # Step 3: Load query vector (same as basic version)
    q_ptrs = Q_ptr + query_idx * d_model + dim_offsets
    query = tl.load(q_ptrs, mask=dim_mask, other=0.0)

    # Step 4: THE VECTORIZATION MAGIC - Load ALL keys at once!
    # This is where we get our speedup

    # Broadcasting explanation:
    # seq_offsets[:, None] creates shape [BLOCK_SIZE_SEQ, 1] - column vector
    # dim_offsets[None, :] creates shape [1, BLOCK_SIZE_DIM] - row vector
    # Broadcasting gives us [BLOCK_SIZE_SEQ, BLOCK_SIZE_DIM] - 2D grid of addresses!
    k_ptrs = K_ptr + seq_offsets[:, None] * d_model + dim_offsets[None, :]
    all_keys = tl.load(k_ptrs, mask=seq_mask[:, None] & dim_mask[None, :], other=0.0)

    # Result: all_keys has shape [seq_len, d_model] - ALL key vectors loaded at once!

    # Step 5: Vectorized attention score computation
    # Instead of computing one dot product at a time, compute ALL at once

    # query has shape [d_model]
    # query[None, :] reshapes to [1, d_model] for broadcasting
    # all_keys has shape [seq_len, d_model]
    # Broadcasting: [1, d_model] * [seq_len, d_model] = [seq_len, d_model]
    # Sum over dim=1: [seq_len, d_model] -> [seq_len] - all scores at once!
    scores = tl.sum(query[None, :] * all_keys, axis=1) * scale

    # Apply masking (same as basic version)
    scores = tl.where(seq_mask, scores, -float('inf'))

    # Step 6: Softmax (unchanged from basic - already vectorized)
    max_score = tl.max(scores, axis=0)
    exp_scores = tl.exp(scores - max_score)
    exp_scores = tl.where(seq_mask, exp_scores, 0.0)
    sum_exp = tl.sum(exp_scores, axis=0)
    attn_weights = exp_scores / sum_exp

    # Step 7: Vectorized output computation - Another speedup!
    # Load ALL values at once (same pattern as keys)
    v_ptrs = V_ptr + seq_offsets[:, None] * d_model + dim_offsets[None, :]
    all_values = tl.load(v_ptrs, mask=seq_mask[:, None] & dim_mask[None, :], other=0.0)

    # Vectorized weighted sum:
    # attn_weights has shape [seq_len]
    # attn_weights[:, None] reshapes to [seq_len, 1] for broadcasting
    # all_values has shape [seq_len, d_model]
    # Broadcasting: [seq_len, 1] * [seq_len, d_model] = [seq_len, d_model]
    # Sum over dim=0: [seq_len, d_model] -> [d_model] - final output!
    output = tl.sum(attn_weights[:, None] * all_values, axis=0)

    # Step 8: Store result (same as basic)
    o_ptrs = Output_ptr + query_idx * d_model + dim_offsets
    tl.store(o_ptrs, output, mask=dim_mask)


# ============================================================================
# CLEAN WRAPPER FUNCTION
# ============================================================================

def vectorized_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
    """
    Vectorized attention with dynamic configuration.

    This function automatically adjusts block sizes based on your input dimensions
    and provides the same interface as PyTorch attention.
    """
    seq_len, d_model = Q.shape
    assert K.shape == V.shape == (seq_len, d_model), "Q, K, V must have same shape"

    # Create output
    output = torch.empty_like(Q)
    scale = 1.0 / math.sqrt(d_model)

    # Auto-configure block sizes
    config = VectorizedConfig()
    BLOCK_SIZE_SEQ, BLOCK_SIZE_DIM = config.adjust_for_problem_size(seq_len, d_model)

    # Launch kernel (one thread per query position)
    grid = (seq_len,)

    vectorized_attention_kernel[grid](
        Q, K, V, output,
        seq_len, d_model, scale,
        BLOCK_SIZE_SEQ, BLOCK_SIZE_DIM
    )

    return output


# ============================================================================
# EDUCATIONAL ANALYSIS FUNCTIONS
# ============================================================================

def explain_vectorization_benefits():
    """Detailed explanation of why vectorization helps"""
    print("üöÄ Why Vectorization Makes Such a Big Difference")
    print("=" * 55)
    print("""
    MEMORY ACCESS COMPARISON:

    Basic Version (inefficient):
    For each query position:
      - Load 1 key vector  (128 bytes)
      - Compute 1 dot product
      - Repeat 512 times
      - Load 1 value vector (128 bytes)
      - Compute 1 weighted value
      - Repeat 512 times
    Total: 512 √ó 2 √ó 128 = 131,072 bytes loaded PER QUERY

    Vectorized Version (efficient):
    For each query position:
      - Load ALL key vectors   (512 √ó 128 = 65,536 bytes)
      - Compute ALL dot products in parallel
      - Load ALL value vectors (512 √ó 128 = 65,536 bytes)
      - Compute weighted sum in parallel
    Total: 131,072 bytes loaded PER QUERY (same total)

    THE DIFFERENCE:
    ‚úÖ Vectorized: 2 large, efficient memory operations
    ‚ùå Basic: 1,024 small, inefficient memory operations

    GPU UTILIZATION:
    - Basic: Uses 1 CUDA core at a time (sequential)
    - Vectorized: Uses hundreds of CUDA cores (parallel)

    MEMORY COALESCING:
    - Basic: Random memory access pattern (cache misses)
    - Vectorized: Sequential memory access (cache hits)

    Result: Same work, much more efficient execution!
    """)


def benchmark_comparison():
    """Comprehensive performance comparison"""
    print("\n‚ö° Performance Comparison: Basic vs Vectorized vs PyTorch")
    print("=" * 60)

    # Test different problem sizes
    test_configs = [
        (128, 64),    # Small
        (256, 128),   # Medium
        (512, 128),   # Large
        (1024, 256),  # Very large
    ]

    results = []

    for seq_len, d_model in test_configs:
        print(f"\nTesting {seq_len}√ó{d_model} (seq_len √ó d_model)")
        print("-" * 30)

        # Create test data
        Q = torch.randn(seq_len, d_model, device='cuda', dtype=torch.float32)
        K = torch.randn(seq_len, d_model, device='cuda', dtype=torch.float32)
        V = torch.randn(seq_len, d_model, device='cuda', dtype=torch.float32)

        # Warm up GPU
        for _ in range(10):
            _ = vectorized_attention(Q, K, V)
            _ = basic_attention(Q, K, V)

        # Benchmark basic attention
        torch.cuda.synchronize()
        start = time.perf_counter()
        for _ in range(100):
            output_basic = basic_attention(Q, K, V)
        torch.cuda.synchronize()
        basic_time = (time.perf_counter() - start) / 100

        # Benchmark vectorized version
        torch.cuda.synchronize()
        start = time.perf_counter()
        for _ in range(100):
            output_vec = vectorized_attention(Q, K, V)
        torch.cuda.synchronize()
        vectorized_time = (time.perf_counter() - start) / 100

        # Benchmark PyTorch for reference
        scale = 1.0 / math.sqrt(d_model)
        torch.cuda.synchronize()
        start = time.perf_counter()
        for _ in range(100):
            scores = torch.matmul(Q, K.transpose(-2, -1)) * scale
            attn = torch.softmax(scores, dim=-1)
            output_torch = torch.matmul(attn, V)
        torch.cuda.synchronize()
        pytorch_time = (time.perf_counter() - start) / 100

        # Check correctness
        max_diff_vec = torch.max(torch.abs(output_vec - output_torch)).item()
        max_diff_basic = torch.max(torch.abs(output_basic - output_torch)).item()
        is_correct_vec = torch.allclose(output_vec, output_torch, rtol=1e-3, atol=1e-3)
        is_correct_basic = torch.allclose(output_basic, output_torch, rtol=1e-3, atol=1e-3)

        speedup_vec_vs_pytorch = pytorch_time / vectorized_time
        speedup_vec_vs_basic = basic_time / vectorized_time
        speedup_basic_vs_pytorch = pytorch_time / basic_time

        print(f"Basic:      {basic_time*1000:.2f} ms")
        print(f"Vectorized: {vectorized_time*1000:.2f} ms")
        print(f"PyTorch:    {pytorch_time*1000:.2f} ms")
        print(f"Speedups:")
        print(f"  Vectorized vs Basic:   {speedup_vec_vs_basic:.2f}x {'üöÄ' if speedup_vec_vs_basic > 1.2 else 'üêå'}")
        print(f"  Vectorized vs PyTorch: {speedup_vec_vs_pytorch:.2f}x {'üöÄ' if speedup_vec_vs_pytorch > 0.8 else 'üêå'}")
        print(f"  Basic vs PyTorch:      {speedup_basic_vs_pytorch:.2f}x {'üöÄ' if speedup_basic_vs_pytorch > 0.8 else 'üêå'}")
        print(f"Correctness:")
        print(f"  Basic:      {is_correct_basic} {'‚úÖ' if is_correct_basic else '‚ùå'} (max diff: {max_diff_basic:.6f})")
        print(f"  Vectorized: {is_correct_vec} {'‚úÖ' if is_correct_vec else '‚ùå'} (max diff: {max_diff_vec:.6f})")

        results.append({
            'config': f"{seq_len}√ó{d_model}",
            'basic_time': basic_time * 1000,
            'vectorized_time': vectorized_time * 1000,
            'pytorch_time': pytorch_time * 1000,
            'speedup_vec_vs_basic': speedup_vec_vs_basic,
            'speedup_vec_vs_pytorch': speedup_vec_vs_pytorch,
            'speedup_basic_vs_pytorch': speedup_basic_vs_pytorch,
            'correct_basic': is_correct_basic,
            'correct_vectorized': is_correct_vec
        })

    # Summary
    print(f"\nüìä SUMMARY:")
    print("-" * 20)
    avg_speedup_vec_vs_basic = np.mean([r['speedup_vec_vs_basic'] for r in results])
    avg_speedup_vec_vs_pytorch = np.mean([r['speedup_vec_vs_pytorch'] for r in results])
    avg_speedup_basic_vs_pytorch = np.mean([r['speedup_basic_vs_pytorch'] for r in results])
    all_correct = all(r['correct_basic'] and r['correct_vectorized'] for r in results)
    print(f"Average speedups:")
    print(f"  Vectorized vs Basic:   {avg_speedup_vec_vs_basic:.2f}x")
    print(f"  Vectorized vs PyTorch: {avg_speedup_vec_vs_pytorch:.2f}x")
    print(f"  Basic vs PyTorch:      {avg_speedup_basic_vs_pytorch:.2f}x")
    print(f"All tests correct: {'‚úÖ' if all_correct else '‚ùå'}")

    return results


def memory_access_analysis():
    """Analyze memory access patterns"""
    print("\nüß† Memory Access Pattern Analysis")
    print("=" * 40)

    seq_len, d_model = 512, 128

    # Theoretical analysis
    total_memory_per_query = 2 * seq_len * d_model * 4  # 2 loads (K,V) √ó seq_len √ó d_model √ó 4 bytes
    total_memory_mb = (total_memory_per_query * seq_len) / (1024 * 1024)

    print(f"Problem size: {seq_len} sequences √ó {d_model} dimensions")
    print(f"Memory per query: {total_memory_per_query / 1024:.1f} KB")
    print(f"Total memory for all queries: {total_memory_mb:.1f} MB")
    print(f"")
    print("Memory access efficiency:")
    print("- Basic version: 1,024 small loads per query (poor cache utilization)")
    print("- Vectorized: 2 large loads per query (excellent cache utilization)")
    print("- Improvement: ~10-50x better memory efficiency")


# ============================================================================
# VISUALIZATION
# ============================================================================

def create_performance_visualization(results):
    """Create visual comparison of results"""
    if not results:
        return

    configs = [r['config'] for r in results]
    basic_times = [r['basic_time'] for r in results]
    vec_times = [r['vectorized_time'] for r in results]
    torch_times = [r['pytorch_time'] for r in results]
    speedups_vec_vs_basic = [r['speedup_vec_vs_basic'] for r in results]
    speedups_vec_vs_pytorch = [r['speedup_vec_vs_pytorch'] for r in results]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))

    # Plot 1: Timing comparison
    x = np.arange(len(configs))
    width = 0.25

    bars1 = ax1.bar(x - width, basic_times, width, label='Basic Triton', color='#ff7f0e', alpha=0.8)
    bars2 = ax1.bar(x, vec_times, width, label='Vectorized Triton', color='#2ca02c', alpha=0.8)
    bars3 = ax1.bar(x + width, torch_times, width, label='PyTorch', color='#1f77b4', alpha=0.8)

    ax1.set_xlabel('Problem Size (seq_len √ó d_model)')
    ax1.set_ylabel('Time (milliseconds)')
    ax1.set_title('Performance Comparison: Basic vs Vectorized vs PyTorch')
    ax1.set_xticks(x)
    ax1.set_xticklabels(configs)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Add value labels on bars
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            ax1.annotate(f'{height:.1f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom', fontsize=8)

    # Plot 2: Speedup factors
    x2 = np.arange(len(configs))
    width2 = 0.35

    bars4 = ax2.bar(x2 - width2/2, speedups_vec_vs_basic, width2,
                   label='Vectorized vs Basic', color='#d62728', alpha=0.8)
    bars5 = ax2.bar(x2 + width2/2, speedups_vec_vs_pytorch, width2,
                   label='Vectorized vs PyTorch', color='#9467bd', alpha=0.8)

    ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Break-even (1.0x)')
    ax2.set_xlabel('Problem Size')
    ax2.set_ylabel('Speedup Factor')
    ax2.set_title('Vectorized Triton Speedups\n(Higher is better)')
    ax2.set_xticks(x2)
    ax2.set_xticklabels(configs)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # Add speedup labels
    for bars, speedups in [(bars4, speedups_vec_vs_basic), (bars5, speedups_vec_vs_pytorch)]:
        for bar, speedup in zip(bars, speedups):
            ax2.annotate(f'{speedup:.2f}x',
                        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom', fontweight='bold', fontsize=8)

    plt.tight_layout()
    plt.show()


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available! This tutorial needs a GPU.")
        exit(1)

    print(f"üéÆ Using GPU: {torch.cuda.get_device_name()}")
    print("\n" + "="*70)
    print("BLOG POST 2: VECTORIZED ATTENTION IN TRITON")
    print("="*70)

    # Educational content
    explain_vectorization_benefits()

    # Performance analysis
    results = benchmark_comparison()

    # Memory analysis
    memory_access_analysis()

    # Visualization
    create_performance_visualization(results)

    print("\nüéØ Key Takeaways from Vectorization:")
    print("- Vectorization eliminates redundant memory operations")
    print("- Broadcasting enables parallel computation on GPU")
    print("- Memory coalescing improves cache utilization")
    print("- Same algorithm, dramatically better performance")
    print("\nüîÆ Coming Next: Blog Post 3 - Advanced Memory Optimizations!")
    print("   (Flash Attention-style techniques, tiling, etc.)")