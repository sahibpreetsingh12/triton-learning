# -*- coding: utf-8 -*-
"""practice-6-7-fused-attention-kernel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14xMkEF0Kq7LYZzMY1hnWRicG7zveTOMs
"""

!pip install triton

import torch
import triton
import triton.language as tl
import math
import time

# ============================================================================
# ONLY ONE KERNEL - THE ONE THAT ACTUALLY WORKS
# ============================================================================

@triton.jit
def attention_kernel(
    Q_ptr, K_ptr, V_ptr, Output_ptr,
    seq_len, d_model, scale,
    BLOCK_SIZE_SEQ: tl.constexpr,
    BLOCK_SIZE_DIM: tl.constexpr
):
    """
    The ONLY attention kernel we need.
    Computes attention for one query position.
    """
    # Which query position this program handles
    query_idx = tl.program_id(0)

    if query_idx >= seq_len:
        return

    # Setup for loading vectors
    dim_offsets = tl.arange(0, BLOCK_SIZE_DIM)
    dim_mask = dim_offsets < d_model

    # Load query vector for this position
    q_ptrs = Q_ptr + query_idx * d_model + dim_offsets
    query = tl.load(q_ptrs, mask=dim_mask, other=0.0)

    # Initialize scores array
    scores = tl.full([BLOCK_SIZE_SEQ], value=-float('inf'), dtype=tl.float32)

    # Compute attention scores for all keys
    for k_idx in range(seq_len):
        if k_idx < BLOCK_SIZE_SEQ:
            # Load key vector
            k_ptrs = K_ptr + k_idx * d_model + dim_offsets
            key = tl.load(k_ptrs, mask=dim_mask, other=0.0)

            # Compute dot product and scale
            score = tl.sum(query * key) * scale

            # Store score at correct position
            scores = tl.where(tl.arange(0, BLOCK_SIZE_SEQ) == k_idx, score, scores)

    # Apply softmax (only to valid positions)
    seq_mask = tl.arange(0, BLOCK_SIZE_SEQ) < seq_len
    scores = tl.where(seq_mask, scores, -float('inf'))

    max_score = tl.max(scores, axis=0)
    scores_shifted = scores - max_score
    exp_scores = tl.exp(scores_shifted)
    exp_scores = tl.where(seq_mask, exp_scores, 0.0)
    sum_exp = tl.sum(exp_scores, axis=0)
    attn_weights = exp_scores / sum_exp

    # Compute output as weighted sum of values
    output = tl.zeros([BLOCK_SIZE_DIM], dtype=tl.float32)

    for v_idx in range(seq_len):
        if v_idx < BLOCK_SIZE_SEQ:
            # Load value vector
            v_ptrs = V_ptr + v_idx * d_model + dim_offsets
            value = tl.load(v_ptrs, mask=dim_mask, other=0.0)

            # Get attention weight
            weight_mask = tl.arange(0, BLOCK_SIZE_SEQ) == v_idx
            weight = tl.sum(tl.where(weight_mask, attn_weights, 0.0))

            # Accumulate weighted value
            output += weight * value

    # Store result
    o_ptrs = Output_ptr + query_idx * d_model + dim_offsets
    tl.store(o_ptrs, output, mask=dim_mask)


# ============================================================================
# ONLY ONE WRAPPER FUNCTION - CALLS THE KERNEL
# ============================================================================

def fused_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
    """
    The ONLY wrapper function we need.
    Sets up parameters and calls the kernel.
    """
    seq_len, d_model = Q.shape
    assert K.shape == V.shape == (seq_len, d_model)

    output = torch.empty_like(Q)
    scale = 1.0 / math.sqrt(d_model)

    # Calculate block sizes
    BLOCK_SIZE_SEQ = 1
    while BLOCK_SIZE_SEQ < seq_len:
        BLOCK_SIZE_SEQ *= 2

    BLOCK_SIZE_DIM = 1
    while BLOCK_SIZE_DIM < d_model:
        BLOCK_SIZE_DIM *= 2

    # Minimum sizes for safety
    BLOCK_SIZE_SEQ = max(BLOCK_SIZE_SEQ, 64)
    BLOCK_SIZE_DIM = max(BLOCK_SIZE_DIM, 64)

    # Launch one program per query position
    grid = (seq_len,)

    attention_kernel[grid](
        Q, K, V, output,
        seq_len, d_model, scale,
        BLOCK_SIZE_SEQ, BLOCK_SIZE_DIM
    )

    return output


# ============================================================================
# TESTING CODE
# ============================================================================

def test_clean_attention():
    """Test the clean, simple implementation"""
    print("üß™ Testing Clean Attention Implementation")
    print("-" * 50)

    # Small test case
    seq_len, d_model = 16, 32
    Q = torch.randn(seq_len, d_model, device='cuda')
    K = torch.randn(seq_len, d_model, device='cuda')
    V = torch.randn(seq_len, d_model, device='cuda')

    # Our implementation
    output_triton = fused_attention(Q, K, V)

    # PyTorch reference
    scale = 1.0 / math.sqrt(d_model)
    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale
    attn = torch.softmax(scores, dim=-1)
    output_pytorch = torch.matmul(attn, V)

    max_diff = torch.max(torch.abs(output_triton - output_pytorch)).item()
    is_correct = torch.allclose(output_triton, output_pytorch, rtol=1e-3, atol=1e-3)

    print(f"Shapes match: {output_triton.shape == output_pytorch.shape}")
    print(f"Max difference: {max_diff:.6f}")
    print(f"Correct: {is_correct} {'‚úÖ' if is_correct else '‚ùå'}")

    if is_correct:
        print("\nüéâ Success! The clean implementation works!")
    else:
        print("\n‚ùå Still have correctness issues...")
        print("Triton sample:", output_triton[0, :4])
        print("PyTorch sample:", output_pytorch[0, :4])


def benchmark_clean():
    """Quick benchmark of the clean version"""
    print("\nüèÉ‚Äç‚ôÇÔ∏è Quick Performance Test")
    print("-" * 30)

    seq_len, d_model = 1024, 1024
    Q = torch.randn(seq_len, d_model, device='cuda')
    K = torch.randn(seq_len, d_model, device='cuda')
    V = torch.randn(seq_len, d_model, device='cuda')

    # Warmup
    for _ in range(50):
        _ = fused_attention(Q, K, V)

    # Time our version
    torch.cuda.synchronize()
    start = time.perf_counter()
    for _ in range(100):
        output_triton = fused_attention(Q, K, V)
    torch.cuda.synchronize()
    triton_time = (time.perf_counter() - start) / 100

    # Time PyTorch
    scale = 1.0 / math.sqrt(d_model)
    torch.cuda.synchronize()
    start = time.perf_counter()
    for _ in range(100):
        scores = torch.matmul(Q, K.transpose(-2, -1)) * scale
        attn = torch.softmax(scores, dim=-1)
        output_pytorch = torch.matmul(attn, V)
    torch.cuda.synchronize()
    pytorch_time = (time.perf_counter() - start) / 100

    speedup = pytorch_time / triton_time

    print(f"Triton:  {triton_time*1000:.3f} ms")
    print(f"PyTorch: {pytorch_time*1000:.3f} ms")
    print(f"Speedup: {speedup:.2f}x {'üöÄ' if speedup > 1 else 'üêå'}")


if __name__ == "__main__":
    test_clean_attention()
    benchmark_clean()

