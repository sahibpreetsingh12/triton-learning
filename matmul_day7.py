# -*- coding: utf-8 -*-
"""matmul-day7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xbNikjtMtSGeGCX6Z63GCNeZXhxy_eGY
"""

!pip install triton
!nvidia-smi

# triton_matmul_template.py
import math
import time
import torch
import triton
import triton.language as tl

# ---------------------------------------
# Tunables (start here)
# ---------------------------------------
BLOCK_M = 64
BLOCK_N = 64
BLOCK_K = 32
NUM_WARPS = 4
NUM_STAGES = 2

@triton.jit
def matmul_kernel(
    A_ptr, B_ptr, C_ptr,
    M, N, K,
    stride_am, stride_ak,    # A: [M, K]
    stride_bk, stride_bn,    # B: [K, N]
    stride_cm, stride_cn,    # C: [M, N]
    BLOCK_M: tl.constexpr,   # tile M
    BLOCK_N: tl.constexpr,   # tile N
    BLOCK_K: tl.constexpr,   # tile K
):
    # 2D program id: which output tile of C this program computes
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    # Offsets for the tile this program computes
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)        # [BM]
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)        # [BN]
    offs_k = tl.arange(0, BLOCK_K)                          # [BK]

    # Pointers to the first A/B tiles for this program
    A_tile_ptr = A_ptr + (offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak)
    B_tile_ptr = B_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn)

    # Accumulator in fp32
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)

    # Main K loop
    k_iter = 0
    while k_iter < K:
        a_mask = (offs_m[:, None] < M) & ((k_iter + offs_k[None, :]) < K)
        b_mask = ((k_iter + offs_k[:, None]) < K) & (offs_n[None, :] < N)

        A_tile = tl.load(A_tile_ptr, mask=a_mask, other=0.0).to(tl.float32)
        B_tile = tl.load(B_tile_ptr, mask=b_mask, other=0.0).to(tl.float32)

        # Microkernel
        acc += tl.dot(A_tile, B_tile)

        # Bump pointers along K
        A_tile_ptr += BLOCK_K * stride_ak
        B_tile_ptr += BLOCK_K * stride_bk
        k_iter += BLOCK_K

    # Write back (downcast optional)
    C_tile_ptr = C_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn)
    c_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)

    # Store as fp16/bf16 if desired; start with fp32 for debugging
    C_out = acc  # .to(tl.float16) or .to(tl.bfloat16) once you're confident
    tl.store(C_tile_ptr, C_out, mask=c_mask)


def triton_matmul(A: torch.Tensor, B: torch.Tensor,
                  block_m=BLOCK_M, block_n=BLOCK_N, block_k=BLOCK_K,
                  num_warps=NUM_WARPS, num_stages=NUM_STAGES,
                  out_dtype=torch.float32) -> torch.Tensor:
    """
    Minimal launcher: C = A @ B
    A: [M, K], B: [K, N]
    Accumulates in fp32; output dtype selectable.
    """
    assert A.is_cuda and B.is_cuda
    assert A.is_contiguous() and B.is_contiguous()
    M, K = A.shape
    Kb, N = B.shape
    assert K == Kb

    C = torch.empty((M, N), device=A.device, dtype=out_dtype)

    grid = (
        triton.cdiv(M, block_m),
        triton.cdiv(N, block_n),
    )

    matmul_kernel[grid](
        A, B, C,
        M, N, K,
        A.stride(0), A.stride(1),
        B.stride(0), B.stride(1),
        C.stride(0), C.stride(1),
        block_m, block_n, block_k,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    return C

# ---------------------------
# Tiny correctness + timing
# ---------------------------
def _time_it(fn, warmup=10, iters=100):
    for _ in range(warmup):
        fn()
    torch.cuda.synchronize()
    t0 = time.time()
    for _ in range(iters):
        fn()
    torch.cuda.synchronize()
    t1 = time.time()
    return (t1 - t0) * 1000.0 / iters  # ms/iter


def _run_once(M=128, N=128, K=128, dtype=torch.float16, out_dtype=torch.float32):
    device = "cuda"
    torch.manual_seed(0)
    A = torch.randn((M, K), device=device, dtype=dtype)
    B = torch.randn((K, N), device=device, dtype=dtype)

    # Triton
    C_tri = triton_matmul(A, B, out_dtype=out_dtype)

    # Reference (compute in fp32 for fairness)
    C_ref = (A.float() @ B.float()).to(out_dtype)

    ok = torch.allclose(C_tri, C_ref, atol=1e-3, rtol=1e-3)
    return ok, C_tri, C_ref


def _bench(M=128, N=128, K=128, dtype=torch.float16, out_dtype=torch.float32):
    device = "cuda"
    A = torch.randn((M, K), device=device, dtype=dtype)
    B = torch.randn((K, N), device=device, dtype=dtype)

    def run_tri():
        triton_matmul(A, B, out_dtype=out_dtype)

    # Triton timing
    tri_ms = _time_it(run_tri, warmup=10, iters=100)

    # PyTorch timing (fp32 compute path for a fair reference)
    def run_torch():
        (A.float() @ B.float())

    torch_ms = _time_it(run_torch, warmup=10, iters=100)
    return tri_ms, torch_ms

if __name__ == "__main__":
    # ---- correctness on a few shapes ----
    for (M, N, K) in [(128,128,128), (256,256,256), (192,128,160), (128,192,160)]:
        ok, _, _ = _run_once(M, N, K, dtype=torch.float16, out_dtype=torch.float32)
        print(f"[check] {M}x{K} @ {K}x{N}: allclose={ok}")

    # ---- quick benchmark ----
    sizes = [(128,128,1024), (256,256,1024)]
    for (M, N, K) in sizes:
        tri_ms, torch_ms = _bench(M, N, K, dtype=torch.float16, out_dtype=torch.float32)
        print(f"[bench] {M}x{K} @ {K}x{N} -> Triton: {tri_ms:.3f} ms | Torch(fp32): {torch_ms:.3f} ms")

    # Notes:
    # - Start with out_dtype=torch.float32 for debugging; once stable, store fp16/bf16.
    # - Next step: add batch/head by passing extra strides and folding B*H into grid or a 3D grid.

