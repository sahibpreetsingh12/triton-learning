# -*- coding: utf-8 -*-
"""matrix-softmax-day-5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zW8gWQc3Vm7I1P4GKaM9BRawAaIVtOw4
"""

!pip install triton==3.0.0
!nvidia-smi

import torch
import triton
import triton.language as tl

print(torch.__version__)
print(triton.__version__)

import torch
import triton
import triton.language as tl


@triton.jit
def softmax_rows_kernel(
    X_ptr, Y_ptr,
    M, N,                      # M = number of rows, N = number of cols
    stride_xm, stride_xn,      # row/col strides for X
    stride_ym, stride_yn,      # row/col strides for Y
    BLOCK_SIZE: tl.constexpr,  # power-of-two ≥ N (or chunk size if you loop)
):
    # each program instance handles ONE row (row = pid_m)
    pid_m = tl.program_id(axis=0)
    if pid_m >= M:
        return

    # column offsets this program will touch (0..BLOCK_SIZE-1)
    offs_n = tl.arange(0, BLOCK_SIZE)

    # pointers to the start of this row for X and Y
    row_x_ptr = X_ptr + pid_m * stride_xm + offs_n * stride_xn
    row_y_ptr = Y_ptr + pid_m * stride_ym + offs_n * stride_yn

    # mask for the tail when N < BLOCK_SIZE
    mask = offs_n < N

    # load row (pad out-of-bounds with -inf for correct max/sum)
    x = tl.load(row_x_ptr, mask=mask, other=-float("inf")).to(tl.float32)

    # numerically stable softmax:
    x_max = tl.max(x, axis=0)
    x = x - x_max
    num = tl.exp(x)
    den = tl.sum(num, axis=0)
    y = num / den

    # store only valid elements
    tl.store(row_y_ptr, y, mask=mask)


def softmax_rows(x: torch.Tensor) -> torch.Tensor:
    """
    x: [M, N] (contiguous or strided), float16/bfloat16/float32
    returns: softmax over last dim (per row), same dtype as x
    """
    assert x.dim() == 2, "expected [M, N]"
    M, N = x.shape
    out = torch.empty_like(x)

    # choose BLOCK_SIZE = next power of two up to 1024 (common max for Triton)
    # this lets us do a single masked pass without looping
    def next_pow2(v):
        return 1 << (v - 1).bit_length()
    BLOCK_SIZE = min(1024, next_pow2(N))

    grid = (triton.cdiv(M, 1),)  # one program per row

    softmax_rows_kernel[grid](
        x, out,
        M, N,
        x.stride(0), x.stride(1),
        out.stride(0), out.stride(1),
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=1 if BLOCK_SIZE <= 128 else 2 if BLOCK_SIZE <= 256 else 4 if BLOCK_SIZE <= 512 else 8,
    )
    return out

# on a CUDA machine / colab with GPU:
x = torch.randn(4096, 384, device="cuda", dtype=torch.float16)
y = softmax_rows(x)

# sanity check vs PyTorch
torch.testing.assert_close(y, torch.softmax(x.float(), dim=-1).to(y.dtype), rtol=1e-2, atol=1e-2)



"""## Benchmarking Triton, Pytorch and Numpy"""

import math, time, statistics as stats
import numpy as np
import torch
import triton
import triton.language as tl
import pandas as pd
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device, torch.cuda.get_device_name(0) if device=="cuda" else "")

@triton.jit
def softmax_rows_kernel(
    X_ptr, Y_ptr,
    M, N,
    stride_xm, stride_xn,
    stride_ym, stride_yn,
    BLOCK_SIZE: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)
    if pid_m >= M:
        return

    offs_n = tl.arange(0, BLOCK_SIZE)
    row_x_ptr = X_ptr + pid_m * stride_xm + offs_n * stride_xn
    row_y_ptr = Y_ptr + pid_m * stride_ym + offs_n * stride_yn

    mask = offs_n < N
    x = tl.load(row_x_ptr, mask=mask, other=-float("inf")).to(tl.float32)

    x_max = tl.max(x, axis=0)
    x = x - x_max
    num = tl.exp(x)
    den = tl.sum(num, axis=0)
    y = num / den

    tl.store(row_y_ptr, y, mask=mask)

def triton_softmax(x: torch.Tensor) -> torch.Tensor:
    assert x.dim() == 2
    M, N = x.shape
    out = torch.empty_like(x)

    def next_pow2(v):
        return 1 << (v - 1).bit_length()
    BLOCK_SIZE = min(1024, next_pow2(N))

    grid = (triton.cdiv(M, 1),)
    softmax_rows_kernel[grid](
        x, out, M, N,
        x.stride(0), x.stride(1),
        out.stride(0), out.stride(1),
        BLOCK_SIZE=BLOCK_SIZE,
        num_warps=1 if BLOCK_SIZE <= 128 else 2 if BLOCK_SIZE <= 256 else 4 if BLOCK_SIZE <= 512 else 8,
    )
    return out

"""## Refrence Implementation Numpy and Pytorch"""

def softmax_numpy(x: np.ndarray) -> np.ndarray:
    # stable softmax over last dim
    x = x - x.max(axis=1, keepdims=True)
    np.exp(x, out=x)
    denom = x.sum(axis=1, keepdims=True)
    x /= denom
    return x

def softmax_torch_gpu(x: torch.Tensor) -> torch.Tensor:
    # x is on CUDA
    return torch.softmax(x, dim=-1)

"""## Benchamrk"""

def bench_fn(fn, setup_args, reps=20, warmup=5, cuda=False):
    # warmup
    for _ in range(warmup):
        out = fn(*setup_args)
        if cuda:
            torch.cuda.synchronize()
    # timed
    times = []
    for _ in range(reps):
        t0 = time.perf_counter()
        out = fn(*setup_args)
        if cuda:
            torch.cuda.synchronize()
        t1 = time.perf_counter()
        times.append((t1 - t0) * 1000.0)  # ms
    return stats.median(times), out

def check_close(a: torch.Tensor, b: torch.Tensor, name_a="A", name_b="B", rtol=1e-2, atol=1e-2):
    torch.testing.assert_close(a, b, rtol=rtol, atol=atol)
    # print(f"{name_a} ≈ {name_b}")

shapes = [
    (1024, 128),
    (1024, 512),
    (4096, 384),
    (4096, 1024),
    (8192, 1024),
]

dtype_torch = torch.float16 if device == "cuda" else torch.float32
results = []

for (M, N) in shapes:
    print(f"\n=== Shape: {M} x {N} ===")

    # --- NumPy (CPU) ---
    x_np = np.random.randn(M, N).astype(np.float32, copy=False)
    t_np, y_np = bench_fn(lambda a: softmax_numpy(a.copy()), (x_np,), reps=10, warmup=3, cuda=False)  # copy() each run

    # --- PyTorch (GPU) ---
    if device == "cuda":
        x_t = torch.randn(M, N, device="cuda", dtype=dtype_torch)
        t_torch, y_torch = bench_fn(softmax_torch_gpu, (x_t,), reps=20, warmup=10, cuda=True)
    else:
        x_t = torch.from_numpy(x_np).to(dtype_torch)
        t_torch, y_torch = bench_fn(lambda a: torch.softmax(a, dim=-1), (x_t,), reps=20, warmup=10, cuda=False)

    # --- Triton (GPU) ---
    if device == "cuda":
        x_tri = x_t  # same tensor to be fair
        t_tri, y_tri = bench_fn(triton_softmax, (x_tri,), reps=20, warmup=10, cuda=True)
    else:
        t_tri, y_tri = float("nan"), None

    # correctness checks (against PyTorch)
    if device == "cuda":
        check_close(y_tri.float(), torch.softmax(x_t.float(), dim=-1), "Triton", "Torch")
    # NumPy vs torch on CPU only (optional small tolerance)
    if device != "cuda":
        check_close(torch.from_numpy(y_np), torch.softmax(x_t.float(), dim=-1), "NumPy", "Torch")

    elems = M * N
    results.append({
        "M": M, "N": N, "Elements": elems,
        "NumPy CPU (ms)": t_np,
        "Torch GPU (ms)" if device=="cuda" else "Torch CPU (ms)": t_torch,
        "Triton GPU (ms)" if device=="cuda" else "Triton GPU (ms)": t_tri,
        "NumPy elems/s": elems / (t_np/1000.0),
        ("Torch elems/s" if device=="cuda" else "Torch elems/s"): elems / (t_torch/1000.0),
        ("Triton elems/s" if device=="cuda" else "Triton elems/s"): (elems / (t_tri/1000.0)) if device=="cuda" else float("nan"),
    })

df = pd.DataFrame(results)
df

# Example: plot times for a single M across different N
focus_M = 4096
dfM = df[df["M"] == focus_M].copy()

plt.figure()
plt.title(f"Softmax time vs N (M={focus_M})")
plt.xlabel("N")
plt.ylabel("Time (ms)")
plt.plot(dfM["N"], dfM["NumPy CPU (ms)"], marker="o", label="NumPy CPU")
if device == "cuda":
    plt.plot(dfM["N"], dfM["Torch GPU (ms)"], marker="o", label="Torch GPU")
    plt.plot(dfM["N"], dfM["Triton GPU (ms)"], marker="o", label="Triton GPU")
else:
    plt.plot(dfM["N"], dfM["Torch CPU (ms)"], marker="o", label="Torch CPU")
plt.legend()
plt.show()

# Example: plot throughput
plt.figure()
plt.title(f"Softmax throughput vs N (M={focus_M})")
plt.xlabel("N")
plt.ylabel("Elements / second")
plt.plot(dfM["N"], dfM["NumPy elems/s"], marker="o", label="NumPy CPU")
if device == "cuda":
    plt.plot(dfM["N"], dfM["Torch elems/s"], marker="o", label="Torch GPU")
    plt.plot(dfM["N"], dfM["Triton elems/s"], marker="o", label="Triton GPU")
else:
    plt.plot(dfM["N"], dfM["Torch elems/s"], marker="o", label="Torch CPU")
plt.legend()
plt.show()

print('sahib')

