# -*- coding: utf-8 -*-
"""triton-day1-2.ipynb


Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AIhYYoIIVIGxIQgDI9v8rSNO7Nf5yKi2
"""

# !pip install -U triton
# !nvidia-smi

import torch
import triton
import triton.language as tl

@triton.jit
def vector_add_kernel(A_ptr, B_ptr, C_ptr, N, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)                        # Which block am I? (1D launch)
    print("process id is ",pid)
    block_start = pid * BLOCK_SIZE                # Start index of my chunk
    offsets = block_start + tl.arange(0, BLOCK_SIZE)  # My thread's element indices

    mask = offsets < N                            # Avoid reading out of bounds
    a = tl.load(A_ptr + offsets, mask=mask)       # Load A[block]
    b = tl.load(B_ptr + offsets, mask=mask)       # Load B[block]
    c = a + b
    tl.store(C_ptr + offsets, c, mask=mask)       # Store result

N = 1024
BLOCK_SIZE = 256

A = torch.randn(N, device='cuda')
B = torch.randn(N, device='cuda')
C = torch.empty_like(A)

vector_add_kernel[(N // BLOCK_SIZE,)](A, B, C, N, BLOCK_SIZE=BLOCK_SIZE)

# Verify
torch.allclose(C, A + B)

"""## day2"""

# Matmul for matrices


import triton
import triton.language as tl
import torch

@triton.jit
def matmul_kernel(
    A_ptr, B_ptr, C_ptr,
    M, N, K,
    stride_am, stride_ak,  # A strides
    stride_bk, stride_bn,  # B strides
    stride_cm, stride_cn,  # C strides
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
  pid_m = tl.program_id(0)  # row block id (for A and C)
  pid_n = tl.program_id(1)  # col block id (for B and C)

  # Compute row and col offsets for this tile of C
  offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
  offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)

  # Initialize accumulator
  acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

  # Loop over tiles in K dimension
  for k in range(0, K, BLOCK_SIZE_K):
      offs_k = k + tl.arange(0, BLOCK_SIZE_K)
	# following pytorch row major format

      # Load tiles from A and B
      A_tile = tl.load(A_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak, mask=(offs_m[:, None] < M) & (offs_k[None, :] < K), other=0.0)
      B_tile = tl.load(B_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn, mask=(offs_k[:, None] < K) & (offs_n[None, :] < N), other=0.0)

      # Accumulate partial dot product
      acc += tl.dot(A_tile, B_tile)

    # Write back to C
  tl.store(C_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))

# Matrix dimensions
M, N, K = 128, 128, 128
BLOCK = 64

# Allocate input and output matrices on GPU
A = torch.randn((M, K), device='cuda', dtype=torch.float32)
B = torch.randn((K, N), device='cuda', dtype=torch.float32)
C = torch.empty((M, N), device='cuda', dtype=torch.float32)

# Define grid dimensions â€” this is what defines program_id(0), program_id(1)
grid = lambda META: (
    triton.cdiv(M, META['BLOCK_SIZE_M']),  # number of row tiles
    triton.cdiv(N, META['BLOCK_SIZE_N']),  # number of col tiles
)

# Launch the kernel
matmul_kernel[grid](
    A, B, C,
    M, N, K,
    A.stride(0), A.stride(1),
    B.stride(0), B.stride(1),
    C.stride(0), C.stride(1),
    BLOCK, BLOCK, BLOCK,
)

C

